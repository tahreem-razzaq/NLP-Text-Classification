The dataset used in this project was thoroughly preprocessed, enabling a fair comparison among all three models. The modelsâ€™ performances were competitive overall. The use of the TF-IDF vectorizer helped normalize the feature distribution in the dataset, allowing models like Naive Bayes to achieve up to 86% accuracy. However, Logistic Regression demonstrated the best performance, likely due to its probabilistic nature. Unlike models that provide only hard class labels, Logistic Regression assigns probability scores to each class (e.g., positive or negative sentiment). A prediction is then made based on whether this probability exceeds a specified threshold (commonly 0.5), allowing for greater flexibility and interpretability in decision-making.
